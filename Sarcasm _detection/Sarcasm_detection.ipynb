{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for smaller datasets please use the simpler model sarcasm_detection_model_CNN_LSTM_DNN_simpler.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from src.data_processing.data_handler import load_glove_model, build_auxiliary_feature\n",
        "\n",
        "sys.path.append('../')\n",
        "\n",
        "import collections\n",
        "import time\n",
        "import numpy\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from keras import backend as K, regularizers\n",
        "from sklearn import metrics\n",
        "from keras.models import model_from_json, load_model\n",
        "from keras.layers.core import Dropout, Dense, Activation, Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "from keras.layers.merge import concatenate, multiply\n",
        "from keras.models import Model\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Input, Reshape, Permute, RepeatVector, Lambda, merge\n",
        "import src.data_processing.data_handler as dh\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class sarcasm_model():\n",
        "    _train_file = None\n",
        "    _test_file = None\n",
        "    _tweet_file = None\n",
        "    _output_file = None\n",
        "    _model_file_path = None\n",
        "    _word_file_path = None\n",
        "    _split_word_file_path = None\n",
        "    _emoji_file_path = None\n",
        "    _vocab_file_path = None\n",
        "    _input_weight_file_path = None\n",
        "    _vocab = None\n",
        "    _line_maxlen = None\n",
        "\n",
        "    def __init__(self):\n",
        "        self._line_maxlen = 30\n",
        "\n",
        "    def attention_3d_block(self, inputs, SINGLE_ATTENTION_VECTOR=False):\n",
        "        # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "        input_dim = int(inputs.shape[2])\n",
        "        a = Permute((2, 1))(inputs)\n",
        "        a = Reshape((input_dim, self._line_maxlen))(a)\n",
        "        # this line is not useful. It's just to know which dimension is what.\n",
        "        a = Dense(self._line_maxlen, activation='softmax')(a)\n",
        "        if SINGLE_ATTENTION_VECTOR:\n",
        "            a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
        "            a = RepeatVector(input_dim)(a)\n",
        "        a_probs = Permute((2, 1), name='attention_vec')(a)\n",
        "        output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
        "        return output_attention_mul\n",
        "\n",
        "    def _build_network(self, vocab_size, maxlen, emb_weights=[], embedding_dimension=50, hidden_units=256,\n",
        "                       batch_size=1):\n",
        "        print('Build model...')\n",
        "\n",
        "        text_input = Input(name='text', shape=(maxlen,))\n",
        "\n",
        "        if (len(emb_weights) == 0):\n",
        "            emb = Embedding(vocab_size, embedding_dimension, input_length=maxlen,\n",
        "                            embeddings_initializer='glorot_normal',\n",
        "                            trainable=True)(text_input)\n",
        "        else:\n",
        "            emb = Embedding(vocab_size, emb_weights.shape[1], input_length=maxlen, weights=[emb_weights],\n",
        "                            trainable=False)(text_input)\n",
        "        emb_dropout = Dropout(0.5)(emb)\n",
        "\n",
        "        lstm_bwd = LSTM(hidden_units, kernel_initializer='he_normal', activation='sigmoid', dropout=0.4,\n",
        "                        go_backwards=True, return_sequences=True)(emb_dropout)\n",
        "        lstm_fwd = LSTM(hidden_units, kernel_initializer='he_normal', activation='sigmoid', dropout=0.4,\n",
        "                        return_sequences=True)(emb_dropout)\n",
        "\n",
        "        lstm_merged = concatenate([lstm_bwd, lstm_fwd])\n",
        "\n",
        "        attention_mul = self.attention_3d_block(lstm_merged)\n",
        "\n",
        "        flat_attention = Flatten()(attention_mul)\n",
        "\n",
        "        aux_input = Input(name='aux', shape=(5,))\n",
        "\n",
        "        merged_aux = concatenate([flat_attention, aux_input], axis=1)\n",
        "\n",
        "\n",
        "        reshaped = Reshape((-1, 1))(merged_aux)\n",
        "\n",
        "        print(reshaped.shape)\n",
        "\n",
        "        cnn1 = Convolution1D(hidden_units, 3, kernel_initializer='he_normal', padding='valid', activation='relu')(\n",
        "            reshaped)\n",
        "        pool1 = MaxPooling1D(pool_size=3)(cnn1)\n",
        "        print(pool1.shape)\n",
        "\n",
        "        cnn2 = Convolution1D(2 * hidden_units, 3, kernel_initializer='he_normal', padding='valid', activation='relu')(\n",
        "            pool1)\n",
        "        pool2 = MaxPooling1D(pool_size=3)(cnn2)\n",
        "        print(pool2.shape)\n",
        "\n",
        "        flat_cnn = Flatten()(pool2)\n",
        "\n",
        "        dnn_1 = Dense(hidden_units)(flat_cnn)\n",
        "        dropout_1 = Dropout(0.25)(dnn_1)\n",
        "        dnn_2 = Dense(2)(dropout_1)\n",
        "        print(dnn_2.shape)\n",
        "\n",
        "        softmax = Activation('softmax')(dnn_2)\n",
        "\n",
        "        model = Model(inputs=[text_input, aux_input], outputs=softmax)\n",
        "\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        print('No of parameter:', model.count_params())\n",
        "\n",
        "        print(model.summary())\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "class train_model(sarcasm_model):\n",
        "    train = None\n",
        "    validation = None\n",
        "    print(\"Loading resource...\")\n",
        "\n",
        "    def __init__(self, train_file, validation_file, word_file_path, split_word_path, emoji_file_path, model_file,\n",
        "                 vocab_file,\n",
        "                 output_file,\n",
        "                 input_weight_file_path=None):\n",
        "        sarcasm_model.__init__(self)\n",
        "\n",
        "        self._train_file = train_file\n",
        "        self._validation_file = validation_file\n",
        "        self._word_file_path = word_file_path\n",
        "        self._split_word_file_path = split_word_path\n",
        "        self._emoji_file_path = emoji_file_path\n",
        "        self._model_file = model_file\n",
        "        self._vocab_file_path = vocab_file\n",
        "        self._output_file = output_file\n",
        "        self._input_weight_file_path = input_weight_file_path\n",
        "\n",
        "        self.load_train_validation_data()\n",
        "\n",
        "        print(self._line_maxlen)\n",
        "        batch_size = 32\n",
        "\n",
        "        # build vocabulary\n",
        "        # truncates words with min freq=1\n",
        "        self._vocab = dh.build_vocab(self.train, min_freq=1)\n",
        "        if ('unk' not in self._vocab):\n",
        "            self._vocab['unk'] = len(self._vocab.keys()) + 1\n",
        "\n",
        "        print(len(self._vocab.keys()) + 1)\n",
        "        print('unk::', self._vocab['unk'])\n",
        "\n",
        "        dh.write_vocab(self._vocab_file_path, self._vocab)\n",
        "\n",
        "        self.train = self.train[:-(len(self.train) % batch_size)]\n",
        "        self.validation = self.validation[:-(len(self.validation) % batch_size)]\n",
        "\n",
        "        # prepares input\n",
        "        X, Y, D, C, A = dh.vectorize_word_dimension(self.train, self._vocab)\n",
        "        X = dh.pad_sequence_1d(X, maxlen=self._line_maxlen)\n",
        "\n",
        "        # prepares input\n",
        "        tX, tY, tD, tC, tA = dh.vectorize_word_dimension(self.validation, self._vocab)\n",
        "        tX = dh.pad_sequence_1d(tX, maxlen=self._line_maxlen)\n",
        "\n",
        "        # embedding dimension\n",
        "        dimension_size = 300\n",
        "        emb_weights = load_glove_model(self._vocab, n=dimension_size,\n",
        "                                       glove_path='/home/aghosh/backups/glove.6B.300d.txt')\n",
        "\n",
        "        # aux inputs\n",
        "        aux_train = build_auxiliary_feature(self.train)\n",
        "        aux_validation = build_auxiliary_feature(self.validation)\n",
        "\n",
        "        # solving class imbalance\n",
        "        ratio = self.calculate_label_ratio(Y)\n",
        "        ratio = [max(ratio.values()) / value for key, value in ratio.items()]\n",
        "        print('class ratio::', ratio)\n",
        "\n",
        "        Y, tY = [np_utils.to_categorical(x) for x in (Y, tY)]\n",
        "\n",
        "        print('train_X', X.shape)\n",
        "        print('train_Y', Y.shape)\n",
        "        print('validation_X', tX.shape)\n",
        "        print('validation_Y', tY.shape)\n",
        "\n",
        "        # trainable true if you want word2vec weights to be updated\n",
        "        # Not applicable in this code\n",
        "        model = self._build_network(len(self._vocab.keys()) + 1, self._line_maxlen, emb_weights, hidden_units=32,\n",
        "                                    embedding_dimension=dimension_size, batch_size=batch_size)\n",
        "\n",
        "        # open(self._model_file + 'model.json', 'w').write(model.to_json())\n",
        "        save_best = ModelCheckpoint(model_file + 'model.json.hdf5', save_best_only=True)\n",
        "        save_all = ModelCheckpoint(self._model_file + 'weights.{epoch:02d}__.hdf5',\n",
        "                                   save_best_only=False)\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
        "\n",
        "        # training\n",
        "        model.fit([X, aux_train], Y, batch_size=batch_size, epochs=10, validation_data=([tX, aux_validation], tY),\n",
        "                  shuffle=True,\n",
        "                  callbacks=[save_best, save_all, early_stopping], class_weight=ratio)\n",
        "\n",
        "    def load_train_validation_data(self):\n",
        "        self.train = dh.loaddata(self._train_file, self._word_file_path, self._split_word_file_path,\n",
        "                                 self._emoji_file_path, normalize_text=True,\n",
        "                                 split_hashtag=True,\n",
        "                                 ignore_profiles=False)\n",
        "        print('Training data loading finished...')\n",
        "\n",
        "        self.validation = dh.loaddata(self._validation_file, self._word_file_path, self._split_word_file_path,\n",
        "                                      self._emoji_file_path,\n",
        "                                      normalize_text=True,\n",
        "                                      split_hashtag=True,\n",
        "                                      ignore_profiles=False)\n",
        "        print('Validation data loading finished...')\n",
        "\n",
        "        if (self._test_file != None):\n",
        "            self.test = dh.loaddata(self._test_file, self._word_file_path, normalize_text=True,\n",
        "                                    split_hashtag=True,\n",
        "                                    ignore_profiles=True)\n",
        "\n",
        "    def get_maxlen(self):\n",
        "        return max(map(len, (x for _, x in self.train + self.validation)))\n",
        "\n",
        "    def write_vocab(self):\n",
        "        with open(self._vocab_file_path, 'w') as fw:\n",
        "            for key, value in self._vocab.iteritems():\n",
        "                fw.write(str(key) + '\\t' + str(value) + '\\n')\n",
        "\n",
        "    def calculate_label_ratio(self, labels):\n",
        "        return collections.Counter(labels)\n",
        "\n",
        "\n",
        "class test_model(sarcasm_model):\n",
        "    test = None\n",
        "    model = None\n",
        "\n",
        "    def __init__(self, model_file, word_file_path, split_word_path, emoji_file_path, vocab_file_path, output_file,\n",
        "                 input_weight_file_path=None):\n",
        "        print('initializing...')\n",
        "        sarcasm_model.__init__(self)\n",
        "\n",
        "        self._model_file_path = model_file\n",
        "        self._word_file_path = word_file_path\n",
        "        self._split_word_file_path = split_word_path\n",
        "        self._emoji_file_path = emoji_file_path\n",
        "        self._vocab_file_path = vocab_file_path\n",
        "        self._output_file = output_file\n",
        "        self._input_weight_file_path = input_weight_file_path\n",
        "\n",
        "        print('test_maxlen', self._line_maxlen)\n",
        "\n",
        "    def load_trained_model(self, model_file='model.json', weight_file='model.json.hdf5'):\n",
        "        start = time.time()\n",
        "        self.__load_model(self._model_file_path + weight_file)\n",
        "        end = time.time()\n",
        "        print('model loading time::', (end - start))\n",
        "\n",
        "    def __load_model(self, model_path):\n",
        "        self.model = load_model(model_path)\n",
        "        print('model loaded from file...')\n",
        "        # self.model.load_weights(model_weight_path)\n",
        "        # print('model weights loaded from file...')\n",
        "\n",
        "    def load_vocab(self):\n",
        "        vocab = defaultdict()\n",
        "        with open(self._vocab_file_path, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                key, value = line.split('\\t')\n",
        "                vocab[key] = value\n",
        "\n",
        "        return vocab\n",
        "\n",
        "    def predict(self, test_file, verbose=False):\n",
        "        try:\n",
        "            start = time.time()\n",
        "            self.test = dh.loaddata(test_file, self._word_file_path, self._split_word_file_path, self._emoji_file_path,\n",
        "                                    normalize_text=True, split_hashtag=True,\n",
        "                                    ignore_profiles=False)\n",
        "            end = time.time()\n",
        "            if (verbose == True):\n",
        "                print('test resource loading time::', (end - start))\n",
        "\n",
        "            self._vocab = self.load_vocab()\n",
        "            print('vocab loaded...')\n",
        "\n",
        "            start = time.time()\n",
        "            tX, tY, tD, tC, tA = dh.vectorize_word_dimension(self.test, self._vocab)\n",
        "            tX = dh.pad_sequence_1d(tX, maxlen=self._line_maxlen)\n",
        "\n",
        "            aux_test = build_auxiliary_feature(self.test)\n",
        "\n",
        "            end = time.time()\n",
        "            if (verbose == True):\n",
        "                print('test resource preparation time::', (end - start))\n",
        "\n",
        "            self.__predict_model([tX, aux_test], self.test)\n",
        "        except Exception as e:\n",
        "            print('Error:', e)\n",
        "            raise\n",
        "\n",
        "    def __predict_model(self, tX, test):\n",
        "        y = []\n",
        "        y_pred = []\n",
        "\n",
        "        # tX = tX[:-len(tX) % 32]\n",
        "        # test = test[:-len(test) % 32]\n",
        "\n",
        "        prediction_probability = self.model.predict_file(tX, batch_size=1, verbose=1)\n",
        "\n",
        "        try:\n",
        "            fd = open(self._output_file + '.analysis', 'w')\n",
        "            for i, (label) in enumerate(prediction_probability):\n",
        "                gold_label = test[i][1]\n",
        "                words = test[i][2]\n",
        "                dimensions = test[i][3]\n",
        "                context = test[i][4]\n",
        "                author = test[i][5]\n",
        "\n",
        "                predicted = numpy.argmax(prediction_probability[i])\n",
        "\n",
        "                y.append(int(gold_label))\n",
        "                y_pred.append(predicted)\n",
        "\n",
        "                fd.write(str(label[0]) + '\\t' + str(label[1]) + '\\t'\n",
        "                         + str(gold_label) + '\\t'\n",
        "                         + str(predicted) + '\\t'\n",
        "                         + ' '.join(words))\n",
        "\n",
        "                fd.write('\\n')\n",
        "\n",
        "            print()\n",
        "\n",
        "            print('accuracy::', metrics.accuracy_score(y, y_pred))\n",
        "            print('precision::', metrics.precision_score(y, y_pred, average='weighted'))\n",
        "            print('recall::', metrics.recall_score(y, y_pred, average='weighted'))\n",
        "            print('f_score::', metrics.f1_score(y, y_pred, average='weighted'))\n",
        "            print('f_score::', metrics.classification_report(y, y_pred))\n",
        "            fd.close()\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    basepath = os.getcwd()[:os.getcwd().rfind('/')]\n",
        "    train_file = basepath + '/resource/train/Train_v1.txt'\n",
        "    validation_file = basepath + '/resource/dev/Dev_v1.txt'\n",
        "    test_file = basepath + '/resource/test/Test_v1.txt'\n",
        "    word_file_path = basepath + '/resource/word_list_freq.txt'\n",
        "    split_word_path = basepath + '/resource/word_split.txt'\n",
        "    emoji_file_path = basepath + '/resource/emoji_unicode_names_final.txt'\n",
        "\n",
        "    output_file = basepath + '/resource/text_model/TestResults.txt'\n",
        "    model_file = basepath + '/resource/text_model/weights/'\n",
        "    vocab_file_path = basepath + '/resource/text_model/vocab_list.txt'\n",
        "\n",
        "    # uncomment for training\n",
        "    tr = train_model(train_file, validation_file, word_file_path, split_word_path, emoji_file_path, model_file,\n",
        "                     vocab_file_path, output_file)\n",
        "\n",
        "    t = test_model(model_file, word_file_path, split_word_path, emoji_file_path, vocab_file_path, output_file)\n",
        "    t.load_trained_model(weight_file='model.json.hdf5')\n",
        "    t.predict(test_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNEwUMZ/9f6mneMRgYgcSlj",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Market_Basket_Analysis_notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
